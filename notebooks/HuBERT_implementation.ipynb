{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDq1ca8TuyxaImR2GoaR5Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01PrathamS/SonicSpeech/blob/main/notebooks/HuBERT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s_TvfC56xvE"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "\n",
        "class ConvFeatureEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(1, 512, kernel_size=10, stride=5, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=5, stride=3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=5, stride=3, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nozhzoY861on"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pos_embedding[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "Qg5h82Gi8TVh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuBERTTransformer(nn.Module):\n",
        "  def __init__(self, d_model=512, n_heads=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
        "    )\n",
        "    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "    self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pos_encoding(x)\n",
        "    return self.transformer(x)"
      ],
      "metadata": {
        "id": "vAFcWx0x8VBX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuBERT(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size=100, d_model=512, num_layers=6):\n",
        "    super().__init__()\n",
        "    self.feature_encoder = ConvFeatureEncoder()\n",
        "    self.projector = nn.Linear(512, d_model)\n",
        "    self.transformer_encoder = HuBERTTransformer(d_model=d_model, num_layers=num_layers)\n",
        "    self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_encoder(x)\n",
        "    x = self.projector(x)\n",
        "    x = self.transformer_encoder(x)\n",
        "    x = self.output_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oNrBdinq87Mm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuBERT(vocab_size = 100)\n",
        "audio_input = torch.randn(1, 1, 16000)\n",
        "output = model(audio_input)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx95nLZ69fKJ",
        "outputId": "1ef11c9b-0089-499d-d7aa-4913f1975176"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 87, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "target = torch.randint(0, 100, (1, 87))  # dummy\n",
        "output = output.view(-1, 100)\n",
        "target = target.view(-1)\n",
        "loss = loss_fn(output, target)\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHNdipiN_gh-",
        "outputId": "de3ebc96-4724-4142-d9d1-a660d3b14b8c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.774441719055176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmVvKaP_AESG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}