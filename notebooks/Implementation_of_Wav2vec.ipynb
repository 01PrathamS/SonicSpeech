{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAf3de3OYwR5S6sKSjQarR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01PrathamS/SonicSpeech/blob/main/notebooks/Implementation_of_Wav2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture\n",
        "\n",
        "\n",
        "\n",
        "1.   Feature Encoder\n",
        "2.   Quantization Module\n",
        "3.   Transformer Encoder\n",
        "4.   Fine Tuning Layer (for ASR)\n",
        "    - can be fine tuned with ctc loss, linear layer mapping features to text\n",
        "\n"
      ],
      "metadata": {
        "id": "LbR6sFsb6EN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1rOzTzwb0wKd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=512):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, hidden_dim, kernel_size=10, stride=5, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_layers(x)"
      ],
      "metadata": {
        "id": "7ePOOvyc02PA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VectorQuantization(nn.Module):\n",
        "    def __init__(self, input_dim=1024, codebook_size=320):\n",
        "        super(VectorQuantization, self).__init__()\n",
        "        self.embedding = nn.Embedding(codebook_size, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.shape\n",
        "        x_flat = x.permute(0, 2, 1).contiguous().view(-1, channels)\n",
        "        distances = torch.cdist(x_flat.unsqueeze(0), self.embedding.weight.unsqueeze(0), p=2).squeeze(0)\n",
        "\n",
        "        nearest_idx = distances.argmin(dim=-1)\n",
        "        quantized = self.embedding(nearest_idx).view(batch_size, seq_len, -1)\n",
        "        return quantized.permute(0, 2, 1)"
      ],
      "metadata": {
        "id": "izzwvSyM1dEg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=1024, num_layers=6, num_heads=8):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=2048, dropout=0.1, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer(x)"
      ],
      "metadata": {
        "id": "z1Jqajn116gi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Wav2Vec2(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=1, hidden_dim=512, quant_codebook_size=320, transformer_layers=6, num_heads=8):\n",
        "        super(Wav2Vec2, self).__init__()\n",
        "        self.feature_encoder = FeatureEncoder(input_dim, hidden_dim)\n",
        "        self.quantizer = VectorQuantization(hidden_dim * 2, quant_codebook_size)\n",
        "        self.transformer_encoder = TransformerEncoder(hidden_dim * 2, transformer_layers, num_heads)\n",
        "        self.output_layer = nn.Linear(hidden_dim * 2, 29)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_encoder(x)\n",
        "        x = self.quantizer(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jYarl7tq2Ttq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Wav2Vec2()\n",
        "x = torch.randn(2, 1, 16000)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0MfDjk-3F7_",
        "outputId": "ee951689-1c24-4df7-d0ac-e481720924f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1600, 29])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class wav2vec2CTC(nn.Module):\n",
        "  def __init__(self, vocab_size=29):\n",
        "    super(wav2vec2CTC, self).__init__()\n",
        "    self.wav2vec2 = Wav2Vec2()\n",
        "    self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def forward(self, x, labels, input_lengths, label_lengths):\n",
        "    logits = self.wav2vec2(x)\n",
        "    logits = logits.permute(1, 0, 2)\n",
        "\n",
        "    loss = self.ctc_loss(\n",
        "        torch.nn.functional.log_softmax(logits, dim=-1),\n",
        "        labels,\n",
        "        input_lengths,\n",
        "        label_lengths\n",
        "    )\n",
        "    return loss, logits\n",
        "\n",
        "model = wav2vec2CTC()"
      ],
      "metadata": {
        "id": "t9c4MBhp3NOc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRnfGExN5xDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}